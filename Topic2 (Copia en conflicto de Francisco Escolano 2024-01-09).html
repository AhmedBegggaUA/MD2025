

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>3. Probability &#8212; Matemáticas Discreta IA</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Topic2';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Markdown Files" href="markdown.html" />
    <link rel="prev" title="2. Combinatorics as counting" href="Topic1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logos.jpeg" class="logo__image only-light" alt="Matemáticas Discreta IA - Home"/>
    <script>document.write(`<img src="_static/logos.jpeg" class="logo__image only-dark" alt="Matemáticas Discreta IA - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Matemáticas Discretas
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Bloque 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bloque1.html">1. Introducción a la asignatura</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Counting and Probability</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Topic1.html">2. Combinatorics as counting</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Probability</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bloque kk</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="markdown.html">4. Markdown Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">5. Content with notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="markdown-notebooks.html">6. Notebooks with MyST Markdown</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Topic2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probability</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-trials">3.1. Independent Trials</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coins-and-dices">3.1.1. Coins and dices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-binomial-distribution">3.1.2. The Binomial distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unimodality">3.1.2.1. Unimodality</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pascal-s-triangle">3.1.2.2. Pascal’s Triangle</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#probable-values-and-fluctuations">3.1.2.3. Probable values and fluctuations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-and-variance">3.1.2.4. Expectation and variance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-inequalities">3.1.2.5. Fundamental inequalities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walks">3.1.2.6. Random walks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-distribution">3.1.2.7. The Normal distribution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probability">
<h1><span class="section-number">3. </span>Probability<a class="headerlink" href="#probability" title="Permalink to this heading">#</a></h1>
<p>Pierre Simon (Marquis of Laplace) in <a class="reference external" href="https://www.informationphilosopher.com/solutions/scientists/laplace/probabilities.html">A Philoshopical Essay on Probabilities</a> comments:</p>
<p><em>Present events are connected with preceding ones
by a tie based upon the evident principle that a thing
cannot occur without a cause which produces it.</em></p>
<p>This leads us directly to “chance”, i.e. we are talking about the “likelihood” of an event <em>in the future</em> based on the present knowledge (also quoting <a class="reference external" href="https://www.feynmanlectures.caltech.edu/I_06.html">The Feynmann Lectures on Physics</a>).</p>
<p>Actually, <span style="color:#469ff8"><strong>probability can be described as</strong> the quantification of chance</span>. In <strong>discrete probability</strong> we talk about a set <span class="math notranslate nohighlight">\(\Omega\)</span> (the <strong>sample set</strong>) containing all the possible <strong>atomic events</strong>. Some examples:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align}
\Omega_1 &amp;= \{H,T\}\; &amp; \text{Results of tossing a coin: Head (H), Tail (T)}\\
\Omega_2 &amp;= \{1,2,\ldots,6\}\; &amp; \text{Results of playing a dice of 6 faces}\\
\Omega_3 &amp;= \{\omega_1,\ldots,\omega_{52!}\} &amp; \text{Ways of suffling a pack of 52 cards}\\
\Omega_4 &amp;= \{\omega_1,\ldots,\omega_{N_{mn}}\} &amp; \text{Number of paths in a grid}\\
\end{align}
\)</span></p>
<p>Note that sometimes we can <em>explicitly name</em> the atomic events but some other times we can only <em>enumerate</em> how many of them do we have. Anyway, in discrete probability we are always <strong>playing with countings</strong>.</p>
<p><span style="color:#469ff8">Actually, the probability of a particular event is the ratio between two counts:</span></p>
<ul class="simple">
<li><p><span style="color:#469ff8">The number of <strong>favorable</strong> cases (to that event). </span></p></li>
<li><p><span style="color:#469ff8">The number of <strong>all cases</strong>.</span></p></li>
</ul>
<p>For atomic events, their probability is simply <span class="math notranslate nohighlight">\(1/|\Omega|\)</span>. However, an event is any proposition that can be evaluated to true or false with a certain probability, such as: “playing a dice returns an even value with probability <span class="math notranslate nohighlight">\(3/6\)</span>”. In this case, the event “even” is not atomic, but a subset <span class="math notranslate nohighlight">\(A\subseteq \Omega\)</span>, where <span class="math notranslate nohighlight">\(A=\{2,4,6\}\)</span>, i.e. the elements in <span class="math notranslate nohighlight">\(\Omega\)</span> that satisfy the logical proposition “return an even value”. We formalize this as follows:</p>
<div class="math notranslate nohighlight">
\[
p(A) = \frac{|A|}{|\Omega|}\;.
\]</div>
<p><strong>Axioms of Probability</strong>. Given an atomic event <span class="math notranslate nohighlight">\(\omega\in\Omega\)</span>, its probability <span class="math notranslate nohighlight">\(p(\omega)\)</span> is a function <span class="math notranslate nohighlight">\(p:\Omega\rightarrow [0,1]\)</span> satisfying:</p>
<p><span class="math notranslate nohighlight">\(
0\le p(\omega)\le 1\; \text{and}\; \sum_{\omega\in\Omega}p(\omega) = 1\;.
\)</span></p>
<p>In addition for <strong>non-atomic events</strong> <span class="math notranslate nohighlight">\(A\)</span> we have:</p>
<p><span class="math notranslate nohighlight">\(
p(A) = \sum_{\omega\in A} p(\omega)= \sum_{\omega\in \Omega} p(\omega)[\omega\in A]\;,
\)</span></p>
<p>where <span class="math notranslate nohighlight">\(p(\omega)[\omega\in A]\)</span> reads: <span class="math notranslate nohighlight">\(p(\omega)\)</span> if <span class="math notranslate nohighlight">\(\omega \in A\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>Finally, for a countable sequence of <strong>disjoint</strong> events <span class="math notranslate nohighlight">\(A_1,A_2,\ldots\)</span> we have the following axiom:</p>
<p><span class="math notranslate nohighlight">\(
p\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} p\left(A_i\right)\;.
\)</span></p>
<p>which is a consequence of the PEI. Actually, if the events are not disjoint we have to consider the full definition of the PEI (excluding-including interesections).</p>
<p><strong>More properties</strong>. As a result of the aforementioned axioms, we have:</p>
<p><span class="math notranslate nohighlight">\(
p(\emptyset) = 0\; \text{and}\; p(\Omega) = 1\;.
\)</span></p>
<p>However, <span class="math notranslate nohighlight">\(\emptyset\)</span> may be not the only event with probability zero. Actually:</p>
<p><span class="math notranslate nohighlight">\(
p(\bar{A}) = 1 - p(A)\; \text{(complement)}\; \text{and}\; A\subseteq B\Rightarrow p(A)\le p(B)\;\text{(monotonicity)}\;
\)</span></p>
<p>Finally, we have the <em>binary</em> PEI:</p>
<p><span class="math notranslate nohighlight">\(
p(A\cup B) = p(A) + p(B) - p(A\cap B)\;.
\)</span></p>
<section id="independent-trials">
<h2><span class="section-number">3.1. </span>Independent Trials<a class="headerlink" href="#independent-trials" title="Permalink to this heading">#</a></h2>
<section id="coins-and-dices">
<h3><span class="section-number">3.1.1. </span>Coins and dices<a class="headerlink" href="#coins-and-dices" title="Permalink to this heading">#</a></h3>
<p>Events can be seen as the output of a given experiment. One of the most simplest experiments is <em>tossing a coin</em>. There are two possible outputs <span class="math notranslate nohighlight">\(\Omega=\{H,T\}\)</span>. If the coin is <em>fair</em>, each of these outputs is <em>equiprobable</em> or <em>equally likely to happen</em>: <span class="math notranslate nohighlight">\(p(H) = p(T) = 1/2\)</span>.</p>
<p>Now assume that the coin experiment can be repeated <span class="math notranslate nohighlight">\(n\)</span> times <strong>under the same conditions</strong>. Each of these repetitions is called a <strong>trial</strong>. So, a natural question to answer is <span style="color:#469ff8">”what is the probability of obtaining, say <span class="math notranslate nohighlight">\(k\)</span> heads after <span class="math notranslate nohighlight">\(n\)</span> trials?”</span>. We can use combinatory to answer to this question. Actually, the solution is:</p>
<div class="math notranslate nohighlight">
\[
P(n,k) = \frac{n\choose k}{2^n}\;.
\]</div>
<p>The number of <em>total cases</em> is <span class="math notranslate nohighlight">\(2^n\)</span>. If we assign <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(T\)</span>, we have <span class="math notranslate nohighlight">\(2^n\)</span> possible numbers of <span class="math notranslate nohighlight">\(n\)</span> bits (permutations with repetition) and each of these numbers concatentates the results of <span class="math notranslate nohighlight">\(n\)</span> trials. In addition, only <span class="math notranslate nohighlight">\({n\choose k}\)</span> cases are <em>favorable</em> since we have  <span class="math notranslate nohighlight">\({n\choose k}\)</span> groups of <span class="math notranslate nohighlight">\(k\)</span> heads in <span class="math notranslate nohighlight">\(n\)</span> trials.</p>
<p>Actually, we can better understand the above rationale by reformulating <span class="math notranslate nohighlight">\(P(n,k)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
P(n,k) = {n\choose k}\left(\frac{1}{2}\right)^n\;.
\]</div>
<p>This means that the probability of any trial is <span class="math notranslate nohighlight">\(1/2\)</span> and all the <span class="math notranslate nohighlight">\(n\)</span> trials are <strong>independent</strong>. Intuitively, statistical independence means that a trial does not influence the following one (same experimental conditions for any trial). More formally, <span class="math notranslate nohighlight">\(n\)</span> events <span class="math notranslate nohighlight">\(A_1, A_2,\ldots, A_n\)</span> are independent if</p>
<div class="math notranslate nohighlight">
\[
P(A_1\cap A_2\cap\ldots \cap A_n) = p(A_1)p(A_2)\ldots p(A_n) = \prod_{i=1}^np(A_i)\;.
\]</div>
<p>Then, the probability of obtaining a given sequence say <span class="math notranslate nohighlight">\(HHHTT\)</span> (<span class="math notranslate nohighlight">\(n=5\)</span>) is</p>
<div class="math notranslate nohighlight">
\[
p(HHHTT) = p(H)p(H)p(H)p(T)p(T)=\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2} = \frac{1}{2^5}=\frac{1}{32}\;.
\]</div>
<p>Actually, all sequences with <span class="math notranslate nohighlight">\(n=5\)</span> are equiprobable. However, what makes <span class="math notranslate nohighlight">\(HHHTT\)</span> different from the others is the fact that <em>we have <span class="math notranslate nohighlight">\(3\)</span> <span class="math notranslate nohighlight">\(H\)</span>s (and consequently <span class="math notranslate nohighlight">\(5-3=2\)</span> heads)</em>. If we want to highlight this fact, we should <em>group</em> all sequences of <span class="math notranslate nohighlight">\(n=5\)</span> trials having <span class="math notranslate nohighlight">\(3\)</span> <span class="math notranslate nohighlight">\(H\)</span>s (or equivalently <span class="math notranslate nohighlight">\(2\)</span> <span class="math notranslate nohighlight">\(T\)</span>s). Some hints:</p>
<ul class="simple">
<li><p><em>Order matters</em>. Strictly speaking, sequences such as <span class="math notranslate nohighlight">\(HHHTT\)</span> and <span class="math notranslate nohighlight">\(TTHHH\)</span> should count twice. Since <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(T\)</span> can be repeated (three times and twice respectively). we have <strong>permutations with repetition</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P_{\sigma}(n) = \frac{n!}{n_1!n_2!}\;\text{where}\; n=5, n_1 = k, n_2=n-k\; 
\]</div>
<ul class="simple">
<li><p>However, we know that</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P_{\sigma}(n) = \frac{n!}{n_1!n_2!} = \frac{n!}{k!(n - k)!} = {n\choose k}
\]</div>
<p>This is where the combinations come from: we have <span class="math notranslate nohighlight">\(n=5\)</span> positions and we need to fill <span class="math notranslate nohighlight">\(3\)</span> of them (order does not matter) with heads. Then, we have <span class="math notranslate nohighlight">\({5\choose 3}\)</span> ways of doing it. It works because the strings <span class="math notranslate nohighlight">\(11100\)</span> and <span class="math notranslate nohighlight">\(00111\)</span> represent different subsets of <span class="math notranslate nohighlight">\(\{0,1\}^{5}\)</span> and what combinations do is to encode subsets (members of the power set).</p>
<p>Therefore, we have used the product rule to decompose the problem in two parts:</p>
<ul class="simple">
<li><p>Compute the probability of a configuration: <span class="math notranslate nohighlight">\(p(HHHTT)\)</span>.</p></li>
<li><p>Compute how many different configurations do you have: <span class="math notranslate nohighlight">\({5\choose 3}\)</span>.</p></li>
</ul>
<p>Then, the probability <span class="math notranslate nohighlight">\(p(\#H=3,5)\)</span> of having <span class="math notranslate nohighlight">\(3\)</span> heads in <span class="math notranslate nohighlight">\(5\)</span> trials is:</p>
<div class="math notranslate nohighlight">
\[
p(\#H=3,5) = {5\choose 3}p(HHHTT) = {5\choose 3}\frac{1}{32} = \frac{10}{32}\;.
\]</div>
<p>In practice, use permutations with repetitions instead of combinations when the outcomes of a experiment are more than <span class="math notranslate nohighlight">\(2\)</span>. We illustrate this case in the following exercise.</p>
<p><span style="color:#347fc9"><strong>Exercise</strong>. Consider the problem of throwing a dice <span class="math notranslate nohighlight">\(n=6\)</span> dices simultaneously. What is the probability of obtaining different results? And all equal?
</span></p>
<p><span style="color:#347fc9"> This is equivalent to <span class="math notranslate nohighlight">\(n\)</span> independent dice trials. Since <span class="math notranslate nohighlight">\(|\Omega|=6\)</span>, we have that <span class="math notranslate nohighlight">\(p(i)=1/6\)</span> for <span class="math notranslate nohighlight">\(i=1,2,\ldots,6\)</span>. Therefore, the probability of a given sequence of <span class="math notranslate nohighlight">\(n\)</span> trials is <span class="math notranslate nohighlight">\((1/6)^n\)</span>. This is the first factor of the product rule (the probability of a given configuration).
</span></p>
<p><span style="color:#347fc9"> The second factor (the number of possible configurations) comes from realizing that each of the <span class="math notranslate nohighlight">\(n\)</span> positions can be filled by different values. Since order matters, we have to count the number of permutations <em>without repetition</em> or <em>permutations with individual repetition</em> (the elements must be different) of <span class="math notranslate nohighlight">\(n\)</span> elements. This leads to <span class="math notranslate nohighlight">\(n!\)</span> and
</span>
<br></br>
<span style="color:#347fc9">
<span class="math notranslate nohighlight">\(
p(\text{All_diff.}) = n!\frac{1}{6^n} = \frac{n!}{1!1!1!1!1!1!} = \frac{6!}{6^6} =\frac{6}{6}\cdot\frac{5}{6}\cdot\frac{4}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}\cdot\frac{1}{6} = 0.015\;.
\)</span>
</span>
<span style="color:#347fc9"> However, there are <span class="math notranslate nohighlight">\(n=6\)</span> configurations where all the dices give the same result:
</span>
<span style="color:#347fc9">
<span class="math notranslate nohighlight">\(
p(\text{All_equal.}) = \frac{n}{6^n} = \frac{1}{6^5} = 0.0001\;. 
\)</span>
</span></p>
</section>
<section id="the-binomial-distribution">
<h3><span class="section-number">3.1.2. </span>The Binomial distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this heading">#</a></h3>
<section id="unimodality">
<h4><span class="section-number">3.1.2.1. </span>Unimodality<a class="headerlink" href="#unimodality" title="Permalink to this heading">#</a></h4>
<p>As we have seen in the previous section, when performing <span class="math notranslate nohighlight">\(n\)</span> independent trials, the odds of some events are higher that those of others. In particular, <strong>extremal events</strong> <span class="math notranslate nohighlight">\(E\)</span> such as maximizing (or minimizing) the appearance of one of the elements of <span class="math notranslate nohighlight">\(\Omega\)</span> have the smallest probability. However, since the probabilities of all events add <span class="math notranslate nohighlight">\(1\)</span>, the bulk of the <em>probability mass</em> should lie in non-extremal events.</p>
<p>In order to see that, we revisite coin tossing, where the two extremal events are <span class="math notranslate nohighlight">\(E=\{\text{All_}Ts, \text{All_}Hs\}\)</span>, both with probability <span class="math notranslate nohighlight">\(1/2^n\)</span>. Then, we have</p>
<div class="math notranslate nohighlight">
\[
P(\text{All_}Ts) = {n\choose 0}\frac{1}{2^n} = {n\choose n}\frac{1}{2^n} = P(\text{All_}Hs) = \frac{1}{2^n}\;.
\]</div>
<p>As a result, the probability of non-extremal events becomes almost <span class="math notranslate nohighlight">\(1\)</span> as <span class="math notranslate nohighlight">\(n\)</span> grows since:</p>
<div class="math notranslate nohighlight">
\[
P(\bar{E}) = 1 - P(E) = 1 - \frac{2}{2^n} = \frac{2^n - 2}{2^n} = \frac{2(2^{n-1} - 1)}{2^n} = \frac{2^{n-1}-1}{2^{n-1}}\;.
\]</div>
<p>Being all the particular configurations equiprobable (i.e. <span class="math notranslate nohighlight">\(1/2^n\)</span>), the fact that <span class="math notranslate nohighlight">\(\lim_{n\rightarrow\infty}P(\bar{E}) = 1\)</span> is due to the number that each particular configuration is repeated.</p>
<p>A closer look to <span class="math notranslate nohighlight">\(2^n = {n\choose 0} + {n\choose 1} + {n\choose 2} + \ldots + {n\choose n}\)</span> gives us the answer. The probability of having <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(H\)</span>s becomes:</p>
<div class="math notranslate nohighlight">
\[
P(n,k) = \frac{{n\choose k}}{{n\choose 0} + {n\choose 1} + {n\choose 2} + \ldots + {n\choose n}}\;,
\]</div>
<p>and due to the symmetry of <span class="math notranslate nohighlight">\({n\choose k} = {n\choose n-k}\)</span>, this probability is going to grow from <span class="math notranslate nohighlight">\(k=0\)</span> until a given <span class="math notranslate nohighlight">\(k=k_{max}\)</span> and then decrease for <span class="math notranslate nohighlight">\(k=n\)</span>. Actually:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(n\)</span> is even, <span class="math notranslate nohighlight">\(k_{max} = \frac{n}{2}\)</span> and <span class="math notranslate nohighlight">\({n\choose 0}&lt;{n\choose 1}&lt;\ldots &lt;{n\choose \frac{n}{2}}&gt;{n\choose \frac{n}{2}+1}&gt;\ldots&gt;{n\choose n}\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(n\)</span> is odd, <span class="math notranslate nohighlight">\(k_{max} = \frac{n-1}{2}=\frac{n+1}{2}\)</span> and <span class="math notranslate nohighlight">\({n\choose 0}&lt;{n\choose 1}&lt;\ldots &lt;{n\choose \frac{n-1}{2}}={n\choose \frac{n+1}{2}}&gt;{n\choose \frac{n}{2}+1}&gt;\ldots&gt;{n\choose n}\)</span>.</p></li>
</ul>
<p>That is, for <span class="math notranslate nohighlight">\(n\)</span> odd we have a double maximum of probability. Anyway the <strong>distribution of probability</strong> among the values <span class="math notranslate nohighlight">\(k=0,1,\ldots,n\)</span> is <strong>unimodal</strong>. In addition, the increase of probability from <span class="math notranslate nohighlight">\(k-1\)</span> to <span class="math notranslate nohighlight">\(k\)</span> before the maximum is given by</p>
<div class="math notranslate nohighlight">
\[
 {n\choose k} = \frac{n!}{k!(n-k)!} = \frac{n-(k-1)}{k}\frac{n!}{(k-1)!\underbrace{(n-(k-1))(n-k)!}_{[n-(k-1)]!}} = \frac{n-(k-1)}{k}{n\choose k-1}
 \]</div>
<p>i.e.</p>
<div class="math notranslate nohighlight">
\[
 \frac{{n\choose k}}{{n\choose k-1}} = \frac{n-(k-1)}{k}\;.
 \]</div>
<p>These properties can be better understood by studying the Pascal’s triangle.</p>
</section>
<section id="pascal-s-triangle">
<h4><span class="section-number">3.1.2.2. </span>Pascal’s Triangle<a class="headerlink" href="#pascal-s-triangle" title="Permalink to this heading">#</a></h4>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Pascal%27s_triangle">Pascal’s Triangle</a> has had many names along the history of mathematics (e.g. Tartaglia Triangle). This construction gives the <span class="math notranslate nohighlight">\({n\choose k}\)</span> for all <span class="math notranslate nohighlight">\(n\)</span>. For instance, <a class="reference internal" href="#ht"><span class="std std-numref">Fig. 3.1</span></a>, each column denotes a value of <span class="math notranslate nohighlight">\(n\)</span> and the coefficients in this column are the <span class="math notranslate nohighlight">\(n+1\)</span> so called <strong>binomial coefficients</strong> for such <span class="math notranslate nohighlight">\(n\)</span>: <span class="math notranslate nohighlight">\({n\choose 0},{n\choose 1},\ldots, {n\choose n}\)</span>.</p>
<figure class="align-center" id="ht">
<a class="reference internal image-reference" href="_images/HT.png"><img alt="_images/HT.png" src="_images/HT.png" style="width: 600px; height: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Binomial coefficients.</span><a class="headerlink" href="#ht" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The triangle is constructed as follows:</p>
<ul class="simple">
<li><p>Start by <span class="math notranslate nohighlight">\(n=0\)</span> and make a trial. We have <span class="math notranslate nohighlight">\({1\choose 0}=1\)</span> ways of obtaining a <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\({1\choose 1}=1\)</span> ways of getting a <span class="math notranslate nohighlight">\(T\)</span>. Then set <span class="math notranslate nohighlight">\(n=1\)</span>.</p></li>
<li><p>From <span class="math notranslate nohighlight">\(H\)</span> we have again <span class="math notranslate nohighlight">\(1\)</span> way of getting a H and one way of getting a <span class="math notranslate nohighlight">\(T\)</span>. However, this also happens from <span class="math notranslate nohighlight">\(T\)</span>. Therefore, after the second experiment we have <span class="math notranslate nohighlight">\(4\)</span> possible outcomes: <span class="math notranslate nohighlight">\(HH, HT, TH, TT\)</span>. Two of these outcomes are extreme event and two of them collapse in the same representation <span class="math notranslate nohighlight">\(HT,TH\)</span> (two ways of getting one <span class="math notranslate nohighlight">\(H\)</span> and one <span class="math notranslate nohighlight">\(T\)</span>). This is way the central node has a value <span class="math notranslate nohighlight">\(2\)</span>.</p></li>
<li><p>Then, we set <span class="math notranslate nohighlight">\(n=2\)</span> and continue…</p></li>
</ul>
<p>Some properties:</p>
<ul class="simple">
<li><p>As noted above, even columns do have a unique maximual coefficients, whereas odd columns have two.</p></li>
<li><p>If the normalize the <span class="math notranslate nohighlight">\(n-\)</span>th column by <span class="math notranslate nohighlight">\(2^n\)</span> we have the <strong>discrete probability distribution</strong> asociated to having <span class="math notranslate nohighlight">\(k=0,1,2,\ldots,n\)</span> heads <span class="math notranslate nohighlight">\(H\)</span>s.</p></li>
<li><p><strong>Extreme events</strong> are always placed (by symmetry) in the main diagonals.</p></li>
<li><p><strong>Non-extreme</strong> or <strong>regular</strong> events begin to fill the distributions as <span class="math notranslate nohighlight">\(n\)</span> increases.</p></li>
<li><p>The coefficients of any <strong>regular event</strong> can be obtained by adding those of their parents in the tree, since:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
{n\choose k} = {n-1\choose k-1} + {n-1\choose k}\;.
\]</div>
<ul class="simple">
<li><p>The binomial coefficient in a node gives the <strong>number of paths</strong> that reach that node from the origin (equivalent to <span class="math notranslate nohighlight">\(\#\Gamma\)</span> in a grid without obstacles). We will come back to this fact later on.</p></li>
<li><p>As <span class="math notranslate nohighlight">\(n\)</span> increases, it becomes more and more clear that the <strong>Binomial probability distribution</strong> is centered on <span class="math notranslate nohighlight">\(n/2\)</span>, where the probability is maximal and then decreases to two tails corresponding to the extremal events. This allows us to intuitively understand the concept of <strong>mean value</strong>, as we will define later on.</p></li>
</ul>
<p>See for instance <a class="reference internal" href="#bern"><span class="std std-numref">Fig. 3.2</span></a>, where we highlight the binomial coefficients after <span class="math notranslate nohighlight">\(n=8\)</span> trials. Check that the highest coefficient at level <span class="math notranslate nohighlight">\(n=8\)</span> is at position <span class="math notranslate nohighlight">\(n/2\)</span> (starting by <span class="math notranslate nohighlight">\(0\)</span>).</p>
<figure class="align-center" id="bern">
<a class="reference internal image-reference" href="_images/Bernouilli.png"><img alt="_images/Bernouilli.png" src="_images/Bernouilli.png" style="width: 600px; height: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Binomial coefficients.</span><a class="headerlink" href="#bern" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Let us express all these things in probabilistic terms!</p>
</section>
<section id="probable-values-and-fluctuations">
<h4><span class="section-number">3.1.2.3. </span>Probable values and fluctuations<a class="headerlink" href="#probable-values-and-fluctuations" title="Permalink to this heading">#</a></h4>
<p>One of the magic elements behind the Pascal’s triangle is that it provides the binomial coefficients, independently of whether we have a <strong>fair coin</strong> or not.</p>
<p>The fair coin is a <em>particular case</em> where the <strong>probability of success</strong> in an independent trial (called <strong>Bernouilli trial</strong>) is <span class="math notranslate nohighlight">\(p=1/2\)</span> (herein, we understand success as “landing on a head”). Consequently, the <strong>probability of failure</strong> (“landing on a tail”) is <span class="math notranslate nohighlight">\(q = 1 - p = 1/2\)</span>. Then, the probability of <span class="math notranslate nohighlight">\(k\)</span> successes after <span class="math notranslate nohighlight">\(n\)</span> trials is more generally given by</p>
<div class="math notranslate nohighlight">
\[
P(n,k)  = {n\choose k}p^k(1-p)^{n-k} = {n\choose k}p^kq^{n-k}\;.
\]</div>
<p>Then, <span class="math notranslate nohighlight">\(P(n,k)\)</span> is the <strong>probability</strong> of having <span class="math notranslate nohighlight">\(k\)</span> successes out of <span class="math notranslate nohighlight">\(n\)</span> trials. This is the solid line in <a class="reference internal" href="#pd"><span class="std std-numref">Fig. 3.3</span></a>, where we have performed <span class="math notranslate nohighlight">\(10,000\)</span> games, each one with <span class="math notranslate nohighlight">\(n=1,000\)</span> and <span class="math notranslate nohighlight">\(p=1/2\)</span>. In the <span class="math notranslate nohighlight">\(x\)</span> axis we place <span class="math notranslate nohighlight">\(k=0,1,\ldots,n\)</span> (the leaves in <a class="reference internal" href="#bern"><span class="std std-numref">Fig. 3.2</span></a> for <span class="math notranslate nohighlight">\(n=1,000\)</span>). In the <span class="math notranslate nohighlight">\(y\)</span> axis we plot (solid line) the <strong>theoretical</strong> <span class="math notranslate nohighlight">\(P(n,k)\)</span> vales. But we also plot (idealy) a bar per <span class="math notranslate nohighlight">\(k\)</span> value. <span style="color:#469ff8">The height of the bars is similar to <span class="math notranslate nohighlight">\(P(n,k)\)</span> but it is not identical. Why?</span> Because we have performed <span class="math notranslate nohighlight">\(10,000\)</span> games. The height of bar <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(\times\)</span> <span class="math notranslate nohighlight">\(10,000\)</span> is the number of games where we have obtained <span class="math notranslate nohighlight">\(k\)</span> heads out of <span class="math notranslate nohighlight">\(n=1,000\)</span> trials. This number (called the <strong>observed number</strong> of successes) is close to <span class="math notranslate nohighlight">\(10,000\cdot P(n,k)\)</span> but it <strong>fluctuates</strong> around it(sometimes it is larger and sometimes it is lower).</p>
<figure class="align-center" id="pd">
<a class="reference internal image-reference" href="_images/PD.png"><img alt="_images/PD.png" src="_images/PD.png" style="width: 700px; height: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.3 </span><span class="caption-text">Theoretical probability vs fluctuations.</span><a class="headerlink" href="#pd" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Obviously, the <strong>most likely</strong> or most probable value of <span class="math notranslate nohighlight">\(k\)</span> is <span class="math notranslate nohighlight">\(n/2=500\)</span>. However, it is not so obvious why the <span class="math notranslate nohighlight">\(k\)</span>s with smallest nonzero <span class="math notranslate nohighlight">\(P(n,k)\)</span> are close to <span class="math notranslate nohighlight">\(440\)</span> and <span class="math notranslate nohighlight">\(560\)</span>. Intuitively, this is related to the extremal events, but these values deserve a deeper mathematical interpretation.</p>
</section>
<section id="expectation-and-variance">
<h4><span class="section-number">3.1.2.4. </span>Expectation and variance<a class="headerlink" href="#expectation-and-variance" title="Permalink to this heading">#</a></h4>
<p><strong>Random variables</strong>. In the above example, each of the <span class="math notranslate nohighlight">\(10,000\)</span> experiments consists of <span class="math notranslate nohighlight">\(n=1,000\)</span> independent trials, each one resulting in landing either in <span class="math notranslate nohighlight">\(H\)</span>s or <span class="math notranslate nohighlight">\(T\)</span>s with probability <span class="math notranslate nohighlight">\(p\)</span>. Then the <strong>sample space</strong> <span class="math notranslate nohighlight">\(\Omega\)</span> is <span class="math notranslate nohighlight">\(\{H,T\}^{n}\)</span>. Well, <span style="color:#469ff8">a <em>random variable</em> <span class="math notranslate nohighlight">\(X\)</span> is a function <span class="math notranslate nohighlight">\(X:\Omega\rightarrow\mathbb{R}\)</span>: <span class="math notranslate nohighlight">\(X(\omega), \omega\in\Omega\)</span></span> , such as the <em>number of <span class="math notranslate nohighlight">\(H\)</span>s</em>. This is indicated by <span class="math notranslate nohighlight">\(X(\omega)=k\)</span>, and <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span> indicates that <span class="math notranslate nohighlight">\(X\)</span> <em>follows</em> a Bernouilli or Binomial distribution.</p>
<p><strong>Expectation</strong>. The expectation of a random variable <span class="math notranslate nohighlight">\(X\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
E(X) = \sum_{\omega\in\Omega}X(\omega)\cdot p(w)= \sum_{x}x\cdot p(X=x)\;.
\]</div>
<p>For <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
E(X) &amp;= \sum_{k=0}^n k\cdot P(n,k)\\
     &amp;= \sum_{k=0}^n k\cdot {n\choose k}p^{k}(1-p)^{n-k}\\
     &amp;= \sum_{k=0}^n n{n-1\choose k-1}p^{k}(1-p)^{n-k}\;\text{since}\;k{n\choose k} = n{n-1\choose k-1}\\
     &amp;= n\sum_{k=0}^n{n-1\choose k-1}p^{k}(1-p)^{n-k}\\
     &amp;= np\sum_{k=1}^n{n-1\choose k-1}p^{k-1}(1-p)^{(n-1)-(k-1)}\\
     &amp;= np\sum_{r=0}^{n-1}{n-1\choose r}p^{r}(1-p)^{(n-1)-r}\;\text{with}\; r=k-1\\
     &amp;= np\;\text{due to the Binomial Theorem}\;.
\end{align}
\end{split}\]</div>
<p><strong>The Binomial Theorem</strong>. Newton’s binomial theorem is behind the Pascal’s triangle and the Binomial distribution. It basically states that the coefficients of expanding <span class="math notranslate nohighlight">\((x+y)^n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is an integer, are the binomial coefficients <span class="math notranslate nohighlight">\({n\choose k}\)</span>, as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
(x+y)^n &amp;= \sum_{j=0}^{n}{n\choose j}x^{n-j}y^{j}\\
        &amp;= {n\choose 0}x^n + {n\choose 1}x^{n-1}y + {n\choose 2}x^{n-2}y^2 + \ldots + {n\choose n-1}xy^{n-1} + {n\choose n}y^{n}\;.
\end{align}
\end{split}\]</div>
<p>Let us observe the theorem for <span class="math notranslate nohighlight">\(n=0,1,2,\ldots.\)</span></p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(n=0\)</span>, we have <span class="math notranslate nohighlight">\((x+y)^0 = {n\choose 0}x^{0-0}y^{0} = 1\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(n=1\)</span>, we have <span class="math notranslate nohighlight">\((x+y) = {n\choose 0}x + {n\choose n}y = 1\cdot x + 1\cdot y\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(n=2\)</span>, <span class="math notranslate nohighlight">\((x+y)^2 = {n\choose 0}x^2 +  {n\choose 1}xy + {n\choose 1}xy + {n\choose n}y^2 = 1\cdot x^2 + 2\cdot xy + 1\cdot y^2\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(n=3\)</span>, <span class="math notranslate nohighlight">\((x+y)^3 = {n\choose 0}x^3 +  {n\choose 1}x^2y + {n\choose 2}xy^2 + {n\choose 1}xy^2 + {n\choose n}y^3 = 1\cdot x^3 + 3\cdot x^2y + 3\cdot xy^2 + 1\cdot x^3\)</span>.</p></li>
</ul>
<p>If you observe the coefficients, they are given by the levels <span class="math notranslate nohighlight">\(n=0,1,2,3,\ldots\)</span> of the Pascal’s Triangle! In other words, herein the <strong>extremal events</strong> are the unit coefficients of <span class="math notranslate nohighlight">\(x^n\)</span> and <span class="math notranslate nohighlight">\(y^n\)</span> respectively. Then, the corresponding <span class="math notranslate nohighlight">\({n\choose k}\)</span> coefficient of <span class="math notranslate nohighlight">\(x^{n-k}y^k\)</span> is just indicating the corresponding product, i.e. how many <span class="math notranslate nohighlight">\(x\)</span>s and how many <span class="math notranslate nohighlight">\(y\)</span>s do we takein each term.</p>
<p>As a result, the way it is built the Pascal’s triangle plays a fundamental role in the <strong>inductive proof</strong> of the theorem. Simply remind the expression <span class="math notranslate nohighlight">\(
{n\choose k} = {n-1\choose k-1} + {n-1\choose k}\;.\)</span>.</p>
<p>Anyway, <span style="color:#469ff8">coming back to the expectation of <span class="math notranslate nohighlight">\(E(X) = np\)</span>, just apply the theorem expressing <span class="math notranslate nohighlight">\(p^{r}(1-p)^{(n-1)-r}\)</span> as <span class="math notranslate nohighlight">\(p^{r}q^{(n-1)-r}\)</span> for the binomial coefficient <span class="math notranslate nohighlight">\({n-1\choose r}\)</span></span>. All we are doing is computing <span class="math notranslate nohighlight">\((x+y)^{n-1}\)</span> where <span class="math notranslate nohighlight">\(x+y = p + q = 1\)</span>. As a result:</p>
<div class="math notranslate nohighlight">
\[
\sum_{r=0}^{n-1}{n-1\choose r}p^{r}q^{(n-1)-r} = (p + q)^{n-1} = 1^{n-1} = 1. 
\]</div>
<p><strong>Variance</strong>. The fact that when <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span> we have <span class="math notranslate nohighlight">\(E(X)=np\)</span> gives us directly the most likely number of successes (see <a class="reference internal" href="#pd"><span class="std std-numref">Fig. 3.3</span></a>). However, to explain the <strong>amount of deviation from the expectation</strong> we rely on the <em>variance</em> which is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
Var(X) =  \sum_{x}(x - E(X))^2\cdot p(X=x)\;.
\]</div>
<p>In this way, <span style="color:#469ff8"><span class="math notranslate nohighlight">\(Var(X)\)</span> is the expectation of the square deviations from <span class="math notranslate nohighlight">\(E(X)\)</span></span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
Var(X) &amp;= E([X-E(X)]^2)\\
       &amp;= E(X^2 + E(X)^2 - 2XE(X))\\
       &amp;= E(X^2) + E(E(X)^2)-2E(XE(X))\\
       &amp;= E(X^2) + E(X)^2 - 2E(X)E(X))\\
       &amp;= E(X^2) + E(X)^2 - 2E(X)^2\\
       &amp;= E(X^2) - E(X)^2\;.
\end{align}
\end{split}\]</div>
<p>One interesting (and simple) way to compute <span class="math notranslate nohighlight">\(Var(X)\)</span> for Binomial variables <span class="math notranslate nohighlight">\(X\)</span> is to realize that <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span> means that <span class="math notranslate nohighlight">\(X = \sum_{i=1}^nY_n\)</span> where <span class="math notranslate nohighlight">\(Y_i\sim Bern(p)\)</span>,i.e. <strong>Bernouilli variables</strong> where the outcomes can be <span class="math notranslate nohighlight">\(1\)</span> (success) or <span class="math notranslate nohighlight">\(0\)</span> (failure).</p>
<p>In particular, if <span class="math notranslate nohighlight">\(Y\sim Bern(p)\)</span> we have:</p>
<div class="math notranslate nohighlight">
\[
E(Y) = \sum_{y}y\cdot p(Y=y) = 1\cdot P(Y=1) + 0\cdot P(Y=0) = p\;.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(X\)</span> is the sum of <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(Y_i\)</span>s: <span class="math notranslate nohighlight">\(E(X) = E(Y_1)+ E(Y_2)+ \ldots + E(Y_n)\)</span>. As <span style="color:#469ff8">the expectation of a sum is the sum of expectations (indepedently of whether the variables are independent or not)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
E(X) = E\left(\sum_{i=1}^nY_i\right) = \sum_{i=1}^n E(Y_i) = np\;.
\]</div>
<p>Now, for computing <span class="math notranslate nohighlight">\(E(Y^2)\)</span>, <span class="math notranslate nohighlight">\(Y\sim Bern(p)\)</span>, we do:</p>
<div class="math notranslate nohighlight">
\[
E(Y^2) = \sum_{y}y^2\cdot p(Y=y) = 1^2\cdot P(Y=1) + 0^2\cdot P(Y=0) = p\;.  
\]</div>
<p>As a result</p>
<div class="math notranslate nohighlight">
\[
Var(Y) = E(Y^2) - E(Y)^2 = p - p^2 = p(1 - p) = pq\;.
\]</div>
<p>Now exploit the fact that <span style="color:#469ff8">the variance of the sum is the sum of variances <strong>when the variables are independent</strong></span>. As this is the case for <span class="math notranslate nohighlight">\(Y_i\sim Bern(p)\)</span>. Then we can calculate</p>
<div class="math notranslate nohighlight">
\[
Var(X) = Var(Y_1 + Y_2 + \ldots + Y_n) = \sum_{i=1}^nVar(Y_i) = npq; 
\]</div>
<p>It is obvious that <span class="math notranslate nohighlight">\(Var(X)\)</span> increases linearly with <span class="math notranslate nohighlight">\(n\)</span>. This simply means that the shape of the distribution (see <a class="reference internal" href="#pd"><span class="std std-numref">Fig. 3.3</span></a>) becomes wider and wider as <span class="math notranslate nohighlight">\(n\)</span> increases.</p>
</section>
<section id="fundamental-inequalities">
<h4><span class="section-number">3.1.2.5. </span>Fundamental inequalities<a class="headerlink" href="#fundamental-inequalities" title="Permalink to this heading">#</a></h4>
<p>Extremal events have small probabilities. In <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span> the probability decays from its maximum value (for <span class="math notranslate nohighlight">\(k=\lfloor np\rfloor\)</span>) until close-to-zero at the extremal events. Such a decay is somewhat described by <span class="math notranslate nohighlight">\(Var(X)\)</span>. However, we have to go deeper in order to characterize the <strong>probability of rare events</strong>.</p>
<p>Firstly, we should measure how the probability decays as we move away from the  expectation. For this task, we rely again on looking <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span> as the sum of <span class="math notranslate nohighlight">\(n\)</span> Bernoulli trials <span class="math notranslate nohighlight">\(Y_i\sim Bern(p)\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(X = Y_1 + Y_2 + \ldots Y_n\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
p(\sum_{i=1}^nY_i - E(X)) = p(X - E(X))
\]</div>
<p><strong>Hoeffding’s theorem</strong> bounds <span class="math notranslate nohighlight">\(p(X - E(X)\ge t)\)</span> for <span class="math notranslate nohighlight">\(t&gt;0\)</span> and <span class="math notranslate nohighlight">\(X\)</span> being the sum of <span class="math notranslate nohighlight">\(n\)</span> independent variables <span class="math notranslate nohighlight">\(Y_i\)</span> satisfying <span class="math notranslate nohighlight">\(a_i\le  Y_i\le b_i\)</span> <em>almost surely</em> or a.s. (i.e. with probability <span class="math notranslate nohighlight">\(1\)</span>), as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(X - E(X)\ge t)&amp;\le \exp\left(-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right)\\
p(|X - E(X)|\ge t)&amp;\le 2\exp\left(-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right)\;.
\end{align}
\end{split}\]</div>
<p>This theorem, simply means that deviating <span class="math notranslate nohighlight">\(t\)</span> units from <span class="math notranslate nohighlight">\(E(X)\)</span>, results in an <strong>exponential decay</strong>. This justifies the exponetial shape of the Binomial distribution as we move from <span class="math notranslate nohighlight">\(E(X)\)</span> towards the extremal events! In particular, for this distribution, where <span class="math notranslate nohighlight">\(0\le  Y_i\le 1\)</span> a.s.,  the Hoeffding’s bounds are:</p>
<div class="math notranslate nohighlight">
\[
p(X - np\ge t)\le \exp\left(-\frac{2t^2}{n}\right)\;\;\; \text{and}\;\;\; p(|X - np|\ge t)\le 2\exp\left(-\frac{2t^2}{n}\right)\;.
\]</div>
<p>Interestingly, the exponential decay is attenuated by <span class="math notranslate nohighlight">\(n\)</span>: the larger <span class="math notranslate nohighlight">\(n\)</span> the slower the decay since the distribution becomes flatter and flatter as <span class="math notranslate nohighlight">\(n\)</span> increases.</p>
<p><strong>Cumulative distribution</strong>. So far, we have been focused on describing random variables in terms of characterizing <span class="math notranslate nohighlight">\(p(X=x)\)</span> (<em>point-mass function</em> or pmf). However, sometimes it is useful to quantify the <strong>bulk of the probability</strong>, for instance between two extremal values <span class="math notranslate nohighlight">\(a&lt;b\)</span>. For <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span>, we now that this bulk is almost <span class="math notranslate nohighlight">\(1\)</span> (but not <em>almost sure</em> unless <span class="math notranslate nohighlight">\(n\rightarrow 1\)</span>, since extremal events exist). However, <span style="color:#469ff8">for <span class="math notranslate nohighlight">\(k:a\le k\le b\)</span>, what is the probability that the number of heads is “less or equal than k”?</span></p>
<p>The usual way to answer this question is to calculate <span class="math notranslate nohighlight">\(p(X\le k)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(X\le k) = \sum_{x\le k}p(X=k)\;.
\]</div>
<p>We can use the <strong>Hoeffding’s bound</strong> to give an idea of this probability, since</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(X\le k) &amp;= 1 - p(X&gt;k)\\
          &amp;= 1 - p(X\ge k+1)\\
          &amp;= 1 - p(X-np\ge (k+1)-np)\\
          &amp;\ge\exp\left(-\frac{((k+1)-np)^2}{n}\right)\;.\\
\end{align}
\end{split}\]</div>
<p>Actually, <span class="math notranslate nohighlight">\(p(X\le k)\)</span> is much larger that the neg-exp. Interestingly, when <span class="math notranslate nohighlight">\(k\approx np\)</span> (close to the expected value), <span class="math notranslate nohighlight">\(p(X\le k)\ge \frac{1}{n}\)</span>, but actually it is <span class="math notranslate nohighlight">\(p(X\le np)=1/2\)</span> due to the symmetry of the distribution. Therefore, the Hoeffding’s bound is a very <strong>conservative bound</strong>.</p>
<p><strong>The Chernoff bound</strong> is one of the tighest bounds for quantifying the probability of rare events. It is formulated as follows. If we have <span class="math notranslate nohighlight">\(n\)</span> independent variables <span class="math notranslate nohighlight">\(Y_1,Y_2,\ldots, Y_n\)</span> with <span class="math notranslate nohighlight">\(p(Y_i=1)=p_i\)</span> and <span class="math notranslate nohighlight">\(p(Y_i=0)=1-p_i\)</span>, <span class="math notranslate nohighlight">\(X = \sum_{i=1}^nY_i\)</span> has expectation <span class="math notranslate nohighlight">\(E(X) = \sum_{i=1}^np_i\)</span> and we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\text{(Lower tail)}\;\;\;\;\;\;\;\;\;\; &amp; p(X-E(X)\le -\lambda)\le \exp\left(-\frac{\lambda^2}{2E(X)}\right)\\
\text{(Upper tail)}\;\;\;\;\;\;\;\;\;\; &amp; p(X-E(X)\ge \lambda)\le \exp\left(-\frac{\lambda^2}{2(E(X)+\lambda/3)}\right)\;.\\
\end{align}
\end{split}\]</div>
<p>The above formulation of the Chernoff bound was proposed in the <a class="reference external" href="https://mathweb.ucsd.edu/~fan/wp/concen.pdf">Survey paper dealing with concetration inequalities</a>. Actually, we have:</p>
<ul class="simple">
<li><p>The lower tail bound holds for all <span class="math notranslate nohighlight">\(\lambda\in [0, E(X)]\)</span> and hence for all real <span class="math notranslate nohighlight">\(\lambda\ge 0\)</span>.</p></li>
<li><p>The upper tail bound, too, holds for all real <span class="math notranslate nohighlight">\(\lambda\ge 0\)</span>.</p></li>
</ul>
<p>Note that this version of <span style="color:#469ff8">the Chernoff bound decays more slowly than the Hoeffding’s bound since the denominator is dominated by <span class="math notranslate nohighlight">\(E(X)\)</span></span>.</p>
<p>Let us now compute the probabilities for the tails in <a class="reference internal" href="#pd"><span class="std std-numref">Fig. 3.3</span></a>: <span class="math notranslate nohighlight">\(440\)</span> and <span class="math notranslate nohighlight">\(560\)</span>. Since <span class="math notranslate nohighlight">\(E(X) = np = 500\)</span>, we have that <span class="math notranslate nohighlight">\(440-500 = -60\)</span> and <span class="math notranslate nohighlight">\(560 - 500 = 60\)</span>, we set <span class="math notranslate nohighlight">\(\lambda = 60\)</span>. Then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(X-E(X)\le -\lambda)\le \exp\left(-\frac{60^2}{1000}\right) = 0.0027\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(X-E(X)\ge \lambda)\le \exp\left(-\frac{60^2}{2(500+60/3)}\right) = 0.0031\)</span>.</p></li>
</ul>
<p>So far, we have characterized the Binomial distribution (concept, expectation, variance and rare events). The Binomial distribution plays a fundamental role in the analysis of AI algorithms for discovering the best solution (whenever possible). These algorithm explore a tree in an “intelligent way”. Before giving an intution of this point, let us introduce an <span style="color:#469ff8">important concept also related with probability and “exploration”: the <strong>random walk</strong></span>. In particular, we will focus at random walks under the Binomial distribution.</p>
</section>
<section id="random-walks">
<h4><span class="section-number">3.1.2.6. </span>Random walks<a class="headerlink" href="#random-walks" title="Permalink to this heading">#</a></h4>
<p>Let us start by defining the following game:</p>
<ul class="simple">
<li><p>Put a traveler at <span class="math notranslate nohighlight">\(x=0\)</span>.</p></li>
<li><p>Toss a coin.</p>
<ul>
<li><p>If the result is head move right: <span class="math notranslate nohighlight">\(x = x + 1\)</span>.</p></li>
<li><p>Otherwise, move left: <span class="math notranslate nohighlight">\(x = x - 1\)</span>.</p></li>
</ul>
</li>
<li><p>Continue until arriving to <span class="math notranslate nohighlight">\(n\)</span> tosses.</p></li>
</ul>
<p><span style="color:#469ff8">The above procedure describes a <strong>one-dimensional random walk</strong> through the <span class="math notranslate nohighlight">\(\mathbb{Z}\)</span></span>. One of the purposes of this game is to answer the question: “How far the traveler gets on average?”</p>
<p>A bit formally, the problem consist of finding the expectation of <span class="math notranslate nohighlight">\(Z\)</span>, the sum of <span class="math notranslate nohighlight">\(n\)</span> independent events <span class="math notranslate nohighlight">\(Y_i\)</span> with output either <span class="math notranslate nohighlight">\(+1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span>. For a fair coin, <span class="math notranslate nohighlight">\(E(Y_i) = 1\cdot p + (-1)\cdot p = 0\)</span> and this implies that <span class="math notranslate nohighlight">\(E(Z)=E(\sum_{i=1}^n Y_i)= \sum_{i=1}^nE(Y_i)=0\)</span>.</p>
<p>Actually, under fairness, we can also answer the question: <span style="color:#469ff8">”What is the probability of landing at a give integer <span class="math notranslate nohighlight">\(z\)</span> after <span class="math notranslate nohighlight">\(n\)</span> steps?”</span>. This can be done by simply quering the Pascal Triangle!
In other words, this problem is equivalent to placing the <span class="math notranslate nohighlight">\(\mathbb{Z}\)</span> line on top on the Pascal’s triangle and aligning <span class="math notranslate nohighlight">\(x=0\)</span> with the top vertex of the triangle. We do that in <a class="reference internal" href="#pascalz"><span class="std std-numref">Fig. 3.4</span></a>, for clarifying that:</p>
<figure class="align-center" id="pascalz">
<a class="reference internal image-reference" href="_images/PascalZ.png"><img alt="_images/PascalZ.png" src="_images/PascalZ.png" style="width: 700px; height: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.4 </span><span class="caption-text">One-dimensional (fair) Random Walk over Pascal’s Triangle.</span><a class="headerlink" href="#pascalz" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>The random walker (RW) may progress towards the left (negative), the right (positive) or coming back home (zero). Landing at a given integer <span class="math notranslate nohighlight">\(z\)</span> is just the probability <span class="math notranslate nohighlight">\(P(z,n)\)</span> of <span class="math notranslate nohighlight">\(z\)</span> successes (if <span class="math notranslate nohighlight">\(z&gt;0\)</span>) or failures (if <span class="math notranslate nohighlight">\(z&lt;0\)</span>).</p></li>
<li><p>In <a class="reference internal" href="#pascalz"><span class="std std-numref">Fig. 3.4</span></a>, we link <span class="math notranslate nohighlight">\(z\in\mathbb{Z}\)</span> with nodes of the respective <em>first levels</em> where we have <span class="math notranslate nohighlight">\(z\)</span> successes (failures), but this line do extend to nodes below them if this property is satisfied: “if we do not have <span class="math notranslate nohighlight">\(z\)</span> successes (failures) yet, maybe we may have them later”. This is basically the gambler’s conflict!</p></li>
<li><p>However, in the long run it is expected that the gambler lands on <span class="math notranslate nohighlight">\(z=0\)</span> (no win - no lose) if its wealth is large enough. This is because <span class="math notranslate nohighlight">\(E(Z) = 0\)</span> is equivalent to <span class="math notranslate nohighlight">\(E(X)=np\)</span> for <span class="math notranslate nohighlight">\(X\sim B(n,p)\)</span>.</p></li>
<li><p>Of course, the hope (land on <span class="math notranslate nohighlight">\(z&gt;0\)</span>) or risk (land on <span class="math notranslate nohighlight">\(z&lt;0\)</span>) is measured by the probability of extremal events as we explained above.</p></li>
<li><p>Logically, if extremal events do happen much more frequently than when predicted by the theory, one may think that the coin is loaded (not fair).</p></li>
</ul>
<p><strong>Regardless of direction</strong>. We have seen that <span class="math notranslate nohighlight">\(E(Z) = 0\)</span> (going forward and backwards is equally likely). However, what happens if we reformulate the original question as follows: “How far the traveler gets on average, <strong>regardless of direction</strong>?”. Answering this question implies computing</p>
<div class="math notranslate nohighlight">
\[
E(Z^2)=E[(\sum_{i=1}^nY_i]^2)\;.
\]</div>
<p>Using the identity (for any <span class="math notranslate nohighlight">\(Z\)</span> given by the sum of <strong>independent identically distributed</strong> or i.i.d. variables):</p>
<div class="math notranslate nohighlight">
\[
Var(Z) = E(Z^2) - E(Z)^2\;.
\]</div>
<p>Hence, we have <span class="math notranslate nohighlight">\(Var(Z) = nVar(Y)\)</span> since the variables <span class="math notranslate nohighlight">\(Y_i\)</span> are i.i.d., so we proceed to measure</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
Var(Y) &amp;= E(Y^2) - E(Y)^2\\
E(Y^2) &amp;= (+1)^2\cdot p + (-1)^2\cdot q = p + q = 1\;.\\
E(Y)^2 &amp;= [(+1)\cdot p + (-1)\cdot q ]^2 = [p - q]^2\;.\\
\end{align}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(p = q\)</span> for a fair coin, we have <span class="math notranslate nohighlight">\(Var(Y) = 1\)</span> and, a result <span class="math notranslate nohighlight">\(Var(Z)=E(Z^2)=n\)</span>. For instance, in <a class="reference internal" href="#rwrand"><span class="std std-numref">Fig. 3.5</span></a> we plot <span class="math notranslate nohighlight">\(100\)</span> fair RWs for <span class="math notranslate nohighlight">\(n=10,000\)</span>. The darkness of the blueness of each walk is proportional to <span class="math notranslate nohighlight">\(Var(Z)=E(Z^2)\)</span> (we have inverted this brightness wrt previous figures for better visualizing extremal paths). Note that most of the paths have “deviations” upper bounded by <span class="math notranslate nohighlight">\(\pm 3\sqrt{Var(Z)} = \pm 3\sqrt{n} = \pm 300\)</span>.</p>
<figure class="align-center" id="rwrand">
<a class="reference internal image-reference" href="_images/RWrand.png"><img alt="_images/RWrand.png" src="_images/RWrand.png" style="width: 800px; height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.5 </span><span class="caption-text">One-dimensional (fair) Random Walk: deviations.</span><a class="headerlink" href="#rwrand" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="the-normal-distribution">
<h4><span class="section-number">3.1.2.7. </span>The Normal distribution<a class="headerlink" href="#the-normal-distribution" title="Permalink to this heading">#</a></h4>
<p>When <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>, the combinatorial nature of <span class="math notranslate nohighlight">\(B(n,p)\)</span> and its links with random walks can be described in a simpler way. The <span class="math notranslate nohighlight">\(B(n,p)\)</span> in the limit becomes another distribution: the well-known <em>Normal</em> or <em>Gaussian</em> distribution.</p>
<p>In this regard, we exploit the Stirling’s approximation rewriten as</p>
<div class="math notranslate nohighlight">
\[
n!\approx \sqrt{2\pi n}\cdot n^n e^{-n}\;.
\]</div>
<p>We plug in this formula in the probability of <span class="math notranslate nohighlight">\(k\)</span> successes, in order to approximate <em>all the factorials</em>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(X=k) &amp; = {n\choose k}p^kq^{n-k}\\
       &amp; = \frac{n!}{k!(n-k)!}p^kq^{n-k}\\
       &amp; \approx \frac{\sqrt{2\pi n}\cdot n^n e^{-n}}{\sqrt{2\pi k}\cdot k^k e^{-k}\sqrt{2\pi (n-k)}\cdot (n-k)^{(n-k)} e^{-(n-k)}}p^kq^{n-k}\\
       &amp; \approx \frac{\sqrt{2\pi n}\cdot n^n e^{-n}}{\sqrt{2\pi k}\sqrt{2\pi (n-k)}\cdot k^k \cdot (n-k)^{(n-k)} e^{-k}e^{-(n-k)}}p^kq^{n-k}\\
       &amp; = \frac{\sqrt{2\pi n}\cdot n^n \cancel{e^{-n}}}{\sqrt{2\pi k}\sqrt{2\pi (n-k)}\cdot k^k \cdot (n-k)^{(n-k)} \cancel{e^{-n}}}p^kq^{n-k}\\
       &amp; = \sqrt{\frac{n}{2\pi k(n-k)}}\cdot\frac{n^n}{k^k \cdot (n-k)^{n-k}} p^kq^{n-k}\\
       &amp; = \sqrt{\frac{n}{2\pi k(n-k)}}\cdot\frac{n^k n^{n-k}}{k^k \cdot (n-k)^{n-k}} p^kq^{n-k}\\
       &amp; = \sqrt{\frac{n}{2\pi k(n-k)}}\cdot\left(\frac{np}{k}\right)^k \cdot\left(\frac{nq}{n-k}\right)^{n-k}\;.\\
\end{align}
\end{split}\]</div>
<p>At this point, it is convenient to formulate the number of succeses <span class="math notranslate nohighlight">\(k\)</span> in terms of deviations <span class="math notranslate nohighlight">\(k = np + z\)</span> from the expectation <span class="math notranslate nohighlight">\(np\)</span> as we did when defining random walks. As a result,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 \begin{align}
 n-k &amp;= n - (np + z) = n - (n(1-q) + z)\\
     &amp;= n - (n - nq + z)\\
     &amp;= nq - z\;.
 \end{align} 
 \end{split}\]</div>
<p>and we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(X=k) &amp;= p(X = np + z)\\ 
       &amp;\approx \sqrt{\frac{n}{2\pi k(n-k)}}\cdot \left(\frac{np}{np+z}\right)^{np + z} \cdot\left(\frac{nq}{nq-z}\right)^{nq - z}\\
       &amp;= \sqrt{\frac{n}{2\pi (np + z)(nq - z)}}\cdot \left(\frac{np}{np + z}\right)^{np + z} \cdot\left(\frac{nq}{nq-z}\right)^{nq - z}\\
\end{align}
\end{split}\]</div>
<p>Since</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{n}{(np + z)(nq - z)} &amp;= \frac{n}{n^2pq - npz + nqz - z^2}\\
                           &amp;= \frac{1}{npq + \frac{-pz + qz - z^2}{n}}\\
                           &amp;\approx \frac{1}{npq}\\
\end{align}
\end{split}\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(X=k) &amp;= p(X = np + z)\\
       &amp;\approx \frac{1}{\sqrt{2\pi\cdot npq}}\cdot \left(\frac{np}{np + z}\right)^{np + z} \cdot\left(\frac{nq}{nq-z}\right)^{nq - z}\\
       &amp;= \frac{1}{\sqrt{2\pi\cdot npq}}\cdot \left(\frac{np + z}{np}\right)^{-(np + z)} \cdot\left(\frac{nq - z}{nq}\right)^{-(nq - z)}\\
       &amp;= \frac{1}{\sqrt{2\pi\cdot npq}}\cdot \left(\frac{np}{np + z}\right)^{np + z} \cdot\left(\frac{nq}{nq - z}\right)^{nq - z}\\
       &amp;= \frac{1}{\sqrt{2\pi\cdot npq}}\cdot \left(1 + \frac{z}{np}\right)^{-(np + z)} \cdot\left(1 - \frac{z}{nq}\right)^{-(nq - z)}\\
       &amp;= \frac{1}{\sqrt{2\pi\cdot npq}}\cdot \left(1 + \frac{z}{np}\right)^{-(np + z)} \cdot\left(1 - \frac{z}{nq}\right)^{-(nq - z)}\;.
\end{align}
\end{split}\]</div>
<p>Now, in order to highlight the exponential shape of <span class="math notranslate nohighlight">\(p(X = np + z)\)</span> let us take logs at both sides</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log p(X=k) &amp;= \log p(X = np + z)\\
            &amp;\approx \log\frac{1}{\sqrt{2\pi\cdot npq}} + \left(1 + \frac{z}{np}\right)^{-(np + z)} + \left(1 - \frac{z}{nq}\right)^{-(nq - z)}\\
            &amp;= C + \log\left(1 + \frac{z}{np}\right)^{-(np + z)} + \log\left(1 - \frac{z}{nq}\right)^{-(nq - z)}\\
            &amp;\approx C -(np + z)\log\left(1 + \frac{z}{np}\right) -(nq - z)\log\left(1 - \frac{z}{nq}\right)\\
            &amp;= C - A - B\\
\end{align}
\end{split}\]</div>
<p>Now, we exploit the Taylor expansions of <span class="math notranslate nohighlight">\(\log(1+x) = x - \frac{1}{2}x^2 + \frac{1}{3}x^3\ldots\)</span> and <span class="math notranslate nohighlight">\(\log(1-x) = -x - \frac{1}{2}x^2 - \frac{1}{3}x^3\ldots\)</span></p>
<p>Taking up to the quadratic terms, for A, B we have</p>
<div class="math notranslate nohighlight">
\[
A = (np + z)\left[\frac{z}{np} - \frac{1}{2}\left(\frac{z}{np}\right)^2\right]\;\;\text{and}\;\; B = (nq - z)\left[-\frac{z}{nq} - \frac{1}{2}\left(\frac{z}{nq}\right)^2\right]
\]</div>
<p>Then, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
A &amp;= \left[np\frac{z}{np} - np\frac{1}{2}\left(\frac{z}{np}\right)^2\right] + 
\left[z\frac{z}{np} - z\frac{1}{2}\left(\frac{z}{np}\right)^2\right]\\
  &amp;= z - \frac{1}{2}\frac{z^2}{np} + \frac{z^2}{np} - \frac{1}{2}\frac{z^3}{(np)^2}\\
  &amp;= z + \frac{1}{2}\frac{z^2}{np} - \frac{1}{2}\frac{z^3}{(np)^2}\\
\end{align}
\end{split}\]</div>
<p>and similarly</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
B &amp;= \left[-nq\frac{z}{nq} - nq\frac{1}{2}\left(\frac{z}{nq}\right)^2\right] - 
\left[-z\frac{z}{nq} - z\frac{1}{2}\left(\frac{z}{nq}\right)^2\right]\\
  &amp;= -z - \frac{1}{2}\frac{z^2}{nq}  + \frac{z^2}{nq} + \frac{1}{2}\frac{z^3}{nq}\\
  &amp;= -z + \frac{1}{2}\frac{z^2}{nq} +\frac{1}{2}\frac{z^3}{(nq)^2}
\end{align}
\end{split}\]</div>
<p>Plugging <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span> in</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\log p(X=k) &amp;= \log p(X = np + z)\\
            &amp;\approx C - A - B\\
            &amp;= \log\frac{1}{\sqrt{2\pi\cdot npq}} - z - \frac{1}{2}\frac{z^2}{np} + \frac{1}{2}\frac{z^3}{(np)^2} + z - \frac{1}{2}\frac{z^2}{nq} -\frac{1}{2}\frac{z^3}{(nq)^2}\\
            &amp;= \log\frac{1}{\sqrt{2\pi\cdot npq}} \cancel{- z} - \frac{1}{2}\frac{z^2}{np} + \frac{1}{2}\frac{z^3}{(np)^2} + \cancel{z} - \frac{1}{2}\frac{z^2}{nq} -\frac{1}{2}\frac{z^3}{(nq)^2}\\
            &amp;= \log\frac{1}{\sqrt{2\pi\cdot npq}}  - \frac{1}{2}\frac{z^2}{np} - \frac{1}{2}\frac{z^2}{nq} + \left(\frac{1}{2}\frac{z^3}{(np)^2}-\frac{1}{2}\frac{z^3}{(nq)^2}\right)\\
            &amp;\approx \log\frac{1}{\sqrt{2\pi\cdot npq}}  - \frac{1}{2}\frac{z^2}{np} - \frac{1}{2}\frac{z^2}{nq} \\
            &amp;= \log\frac{1}{\sqrt{2\pi\cdot npq}}  + \frac{(-q - p)z^2}{2npq}\\ 
            &amp;= \log\frac{1}{\sqrt{2\pi\cdot npq}}  + \frac{(-q - (1-q))z^2}{2npq} \\
            &amp;= \log\frac{1}{\sqrt{2\pi\cdot npq}}  - \frac{z^2}{2npq} \\
\end{split}\]</div>
<p>As a result, if we replace <span class="math notranslate nohighlight">\(z\)</span> by <span class="math notranslate nohighlight">\(k - pq\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\log p(X=k) - \log\frac{1}{\sqrt{2\pi\cdot npq}} =  - \frac{z^2}{2npq}\;,
\]</div>
<p>i.e., the so called  <span style="color:#469ff8"><strong>De Moivre - Laplace theorem</strong> yields the usual expression for the Normal distribution as a limit of the Binomial one</span>:</p>
<div class="math notranslate nohighlight">
\[
p(X=k) =  \frac{1}{\sqrt{2\pi\cdot npq}}\exp\left(\frac{1}{2}\frac{(k-np)^2}{npq}\right)\;,
\]</div>
<p>More generally, as <span class="math notranslate nohighlight">\(np\)</span> is the “mean” of the Binomial, it is renamed as <strong>mean</strong> <span class="math notranslate nohighlight">\(\mu\)</span> of the Normal. Similarly, as <span class="math notranslate nohighlight">\(npq = Var(X)\)</span>, it is renamed the <strong>variance</strong> <span class="math notranslate nohighlight">\(\sigma^2\)</span> of the Normal, whose squere root is the <strong>standard deviation</strong> <span class="math notranslate nohighlight">\(\sqrt{\sigma^2}=\sigma\)</span>.</p>
<p>Therefore, we can say that <span class="math notranslate nohighlight">\(X\sim N(\mu,\sigma^2)\)</span> if its <em>pmf</em> is</p>
<div class="math notranslate nohighlight">
\[
p(X=x) =  \frac{1}{\sqrt{2\pi\cdot \sigma}}\exp\left(\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\right)\;.
\]</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Topic1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Combinatorics as counting</p>
      </div>
    </a>
    <a class="right-next"
       href="markdown.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Markdown Files</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-trials">3.1. Independent Trials</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coins-and-dices">3.1.1. Coins and dices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-binomial-distribution">3.1.2. The Binomial distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unimodality">3.1.2.1. Unimodality</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pascal-s-triangle">3.1.2.2. Pascal’s Triangle</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#probable-values-and-fluctuations">3.1.2.3. Probable values and fluctuations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-and-variance">3.1.2.4. Expectation and variance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-inequalities">3.1.2.5. Fundamental inequalities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walks">3.1.2.6. Random walks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-distribution">3.1.2.7. The Normal distribution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Universidad de Alicante
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>